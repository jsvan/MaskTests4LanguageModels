Roberta-Large handles most masking schemes without problem. The worst masks are alternate spellings of the same word. However, with one epoch of training, there is no masking scheme which is meaningfully different than no masking at all. This shows that the Roberta model robustly keeps track of object identity above a naive usage of unigram word vector representation. Read the ipynb for more. 

![image](https://user-images.githubusercontent.com/9337973/176744925-04a350af-d225-4b8b-9d86-82074c2170df.png)
