{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsvan/MaskTests4LanguageModels/blob/main/Extension_of_Summerwinograndeattempt_to_more_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pmnlcR-R2MD"
      },
      "source": [
        "In this extension I will try out more models, first tuning generic language models on the debiased dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SMvKLP9taL-G"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OyvVPUg5nij4"
      },
      "outputs": [],
      "source": [
        "# Textizer\n",
        "\"\"\"\n",
        "    Each sentence was split on \"_\" placeholder symbol.\n",
        "    Each option was concatenated with the second part of the split, thus transforming each example into two text segment pairs.\n",
        "    Text segment pairs corresponding to correct and incorrect options were marked with True and False labels accordingly.\n",
        "    Text segment pairs were shuffled thereafter.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "def prepare_data(dataset):\n",
        "\n",
        "  # internal function\n",
        "  def prep_ds(dataset):\n",
        "    sentences, answers, o1, o2 = [], [], [], []\n",
        "    for p in dataset:\n",
        "      s1 = p['option1'].join(p['sentence'].split('_'))\n",
        "      s2 = p['option2'].join(p['sentence'].split('_'))\n",
        "      a1 = int(p['answer'] == '1')\n",
        "      a2 = int(p['answer'] == '2')\n",
        "\n",
        "      \n",
        "      sentences.append(s1)\n",
        "      answers.append(a1)\n",
        "      sentences.append(s2)\n",
        "      answers.append(a2)\n",
        "      o1.append(p['option1'])\n",
        "      o2.append(p['option2'])\n",
        "      o1.append(p['option1'])\n",
        "      o2.append(p['option2'])\n",
        "\n",
        "    return {'sentence':sentences, 'labels':answers, 'option1':o1, 'option2':o2}\n",
        "    # end internal function\n",
        "\n",
        "  train = prep_ds(dataset[\"train\"])\n",
        "  test = prep_ds(dataset[\"validation\"])\n",
        "  trainds = Dataset.from_dict( train ).shuffle()\n",
        "  testds = Dataset.from_dict( test ).shuffle()\n",
        "\n",
        "  return {\"train\":trainds, \"test\":testds, \"name\":\"Standard Dataset\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mask_datasets(dataset):\n",
        "\n",
        "  # internal method BEGIN\n",
        "  def mask_copy(dataset):\n",
        "    sentences = []\n",
        "    toprint = 1\n",
        "\n",
        "    for p in dataset:\n",
        "      if toprint > 0:\n",
        "        print(f\"[{p['option1']}], [{p['option2']}], [{p['sentence']}]\")\n",
        "\n",
        "      sentences.append(p['sentence'].replace(p['option1'], 'option1').replace(p['option2'], 'option2'))\n",
        "      \n",
        "      if toprint > 0:\n",
        "        print(sentences[-1])\n",
        "        toprint -= 1\n",
        "\n",
        "    build = {'sentence':sentences, 'labels':dataset['labels'], 'option1':dataset['option1'], 'option2':dataset['option2']}\n",
        "    return Dataset.from_dict(build) # DON'T SHUFFLE\n",
        "  #END\n",
        "  \n",
        "  return {\"train\":mask_copy(dataset['train']), \"test\":mask_copy(dataset['test']), \"name\":\"Masked Dataset\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "1eHIx_YjIVdL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nD8_o4yf9A8j"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "\n",
        "\"\"\" \n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
        "\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"roberta-large\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def test_datasets(tokenizer, std_datasets, masked_datasets, model=False,):\n",
        "\n",
        "  print(\"\\nTESTING TESTING  1 2 3 ...\")\n",
        "  delete = False\n",
        "  if not model:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"DeepPavlov/roberta-large-winogrande\")\n",
        "    delete = True\n",
        "\n",
        "  elif isinstance(model, str):\n",
        "    delete = True\n",
        "    with torch.no_grad():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    model = torch.load(model) # open(model, \"rb\"))\n",
        "\n",
        "    with torch.no_grad():\n",
        "      torch.cuda.empty_cache()\n",
        "  try:\n",
        "    for ds in (std_datasets, masked_datasets):\n",
        "      print(\"\\nPERFORMING\", ds[\"name\"])\n",
        "      \n",
        "      #combinedtraintest = concatenate_datasets([ds['train'], ds['test']])\n",
        "      #encoded_train = combinedtraintest.map(lambda examples: tokenizer(examples['sentence'], padding='max_length'), batched=True) # , return_tensors='pt'\n",
        "      encoded_test = ds['test'].map(lambda examples: tokenizer(examples['sentence'], padding='max_length'), batched=True)\n",
        "\n",
        "      #encoded_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "      encoded_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "      #dataloader_train = torch.utils.data.DataLoader(encoded_train, batch_size=32)\n",
        "      dataloader_test = torch.utils.data.DataLoader(encoded_test, batch_size=32)\n",
        "\n",
        "      device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "      #model.train().to(device)\n",
        "      model.to(device)\n",
        "      #optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
        "\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for i, batch in enumerate(tqdm(dataloader_test)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        #for oo, lab in zip(outputs.logits, batch['labels'] ):\n",
        "        #  print(oo.argmax().item(), lab.items())\n",
        "        correct += sum((int(x.argmax().item() == y.item()) for x, y in zip(outputs.logits, batch['labels'])))\n",
        "        total += len(batch['labels'])\n",
        "        \n",
        "        \n",
        "      print(\"\\nSCORE\", correct / total, '/ 1.00')\n",
        "\n",
        "    if delete:\n",
        "      del model\n",
        "      print(\"Deleted model\")\n",
        "\n",
        "  except Exception as e:\n",
        "    del model\n",
        "    print(e, e.__str__)\n",
        "    print(\"Deleted model\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWCsnqnlRxye"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(trainingset, compareset, tokenizer, model=False, copy=False, epochs=1):\n",
        "  print('\\n\\nTRAIN:\\n', '\\n'.join(trainingset['train']['sentence'][0:10]))\n",
        "  print('\\n\\nLABELS:\\n', '\\n'.join([str(x) for x in trainingset['train']['labels'][0:10]]))\n",
        "\n",
        "  print('\\n\\nTEST:\\n', '\\n'.join(trainingset['test']['sentence'][0:10]))\n",
        "  print('\\n\\nLABELS:\\n', '\\n'.join([str(x) for x in trainingset['test']['labels'][0:10]]))\n",
        "\n",
        "\n",
        "  if copy: # ie, if copy and model\n",
        "    print('DUPLICATING MODEL')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      torch.cuda.empty_cache()\n",
        "    model = torch.load(model) # open(model, \"rb\"))\n",
        "    \n",
        "  elif model and isinstance(model, str): # ie if str(model) AND NOT copy:\n",
        "    print(\"DOWNLOADING MODEL\")\n",
        "    # download new model of name\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model)\n",
        "\n",
        "  try:\n",
        "    encoded_train = trainingset['train'].map(lambda examples: tokenizer(examples['sentence'], padding='max_length'), batched=True) # , return_tensors='pt'\n",
        "    encoded_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    dataloader_train = torch.utils.data.DataLoader(encoded_train, batch_size=32)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
        "    model.train().to(device)\n",
        "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      print(\"EPOCH\", epoch+1, '/', epochs)\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for i, batch in enumerate(tqdm(dataloader_train)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if i < 1:\n",
        "          for oo, lab in zip(outputs.logits, batch['labels'] ):\n",
        "            print(oo.argmax().item(), lab.item())\n",
        "        correct += sum((int(x.argmax().item() == y.item()) for x, y in zip(outputs.logits, batch['labels'])))\n",
        "        total += len(batch['labels'])\n",
        "        \n",
        "      print(\"Score\", correct / total, '/ 1.00')\n",
        "      test_datasets(tokenizer, trainingset, compareset, model)\n",
        "      model.train().to(device)\n",
        "  except Exception as e:\n",
        "    del model\n",
        "    print(e, e.__str__)\n",
        "    print(\"deleted model\")\n",
        "  if copy:\n",
        "    del model\n",
        "  else:\n",
        "    return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JJJenvgKxstY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def mask_copy_1(dataset, unk):\n",
        "  sentences = []\n",
        "  toprint = 1\n",
        "\n",
        "  for p in dataset:\n",
        "    # This is annoying because option1 can be substrings of other words, usually option2. They can also have uppercase letters. \n",
        "    # If one is a substring, then I will cover up the larger word with a temporary mask to not confuse anything else.\n",
        "    sentence = p['sentence']\n",
        "    option1, option2 = re.compile(p['option1'], re.IGNORECASE), re.compile(p['option2'], re.IGNORECASE)\n",
        "    maskedoption1, maskedoption2 = \"OPTION_ONE\", \"OPTION_TWO\"\n",
        "    first_search, second_search, first_mask, second_mask = None, None, None, None\n",
        "\n",
        "    # \"table\" in \"tablecloth\" --> cover bigger one\n",
        "    if len(p['option1']) > len(p['option2']):\n",
        "      # cover option1 first\n",
        "      first_search, second_search = option1, option2\n",
        "      first_mask, second_mask = maskedoption1, maskedoption2\n",
        "    else:\n",
        "      first_search, second_search = option2, option1\n",
        "      first_mask, second_mask = maskedoption2, maskedoption1\n",
        "    \n",
        "    sentence = first_search.sub(first_mask, sentence) #Mask the longer word with OPTION_ mask\n",
        "    sentence = second_search.sub(second_mask, sentence) #then the shorter one\n",
        "\n",
        "    # IT gets kinda confusing which word now to <IGNORE> because it's language and some words appear more than two, more than three, times. \n",
        "    # I think the smartest approach is to assume the final word used is the question word and mask that one. \n",
        "\n",
        "    # Find final word used\n",
        "    # maskedoption1 is the final word\n",
        "    if sentence.rfind(maskedoption1) > sentence.rfind(maskedoption2):\n",
        "      # IGNORE final word\n",
        "      # Convert other word back to original\n",
        "      sentence = sentence.replace(maskedoption1, unk)\n",
        "      sentence = sentence.replace(maskedoption2, p['option2'])\n",
        "    else:\n",
        "      sentence = sentence.replace(maskedoption2, unk)\n",
        "      sentence = sentence.replace(maskedoption1, p['option2'])\n",
        "\n",
        "\n",
        "    if toprint > 0:\n",
        "      print(f\"[{p['option1']}], [{p['option2']}], [{p['sentence']}]\")\n",
        "\n",
        "    sentences.append(sentence)\n",
        "    \n",
        "    if toprint > 0:\n",
        "      print(sentences[-1])\n",
        "      toprint -= 1\n",
        "\n",
        "  build = {'sentence':sentences, 'labels':dataset['labels'], 'option1':dataset['option1'], 'option2':dataset['option2']}\n",
        "  return Dataset.from_dict(build) # DON'T SHUFFLE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mask_datasets_1(dataset, tokenizer):\n",
        "  unk = tokenizer.unk_token\n",
        "  return {\"train\":mask_copy_1(dataset['train'], unk), \"test\":mask_copy_1(dataset['test'], unk), \"name\":\"Double <Unk> Masked Dataset\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pUj9r6mk4lvB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# This masked <unk> tokens on the option which is NOT involved in the question. \n",
        "# If the option involved in the question is wrong, it will need to identify the <unk> token as having importance.\n",
        "def mask_copy_2(dataset, unk):\n",
        "  sentences = []\n",
        "  toprint = 1\n",
        "\n",
        "  for p in dataset:\n",
        "    # This is annoying because option1 can be substrings of other words, usually option2. They can also have uppercase letters. \n",
        "    # If one is a substring, then I will cover up the larger word with a temporary mask to not confuse anything else.\n",
        "    sentence = p['sentence']\n",
        "    option1, option2 = re.compile(p['option1'], re.IGNORECASE), re.compile(p['option2'], re.IGNORECASE)\n",
        "    maskedoption1, maskedoption2 = \"OPTION_ONE\", \"OPTION_TWO\"\n",
        "    first_search, second_search, first_mask, second_mask = None, None, None, None\n",
        "\n",
        "    # \"table\" in \"tablecloth\" --> cover bigger one\n",
        "    if len(p['option1']) > len(p['option2']):\n",
        "      # cover option1 first\n",
        "      first_search, second_search = option1, option2\n",
        "      first_mask, second_mask = maskedoption1, maskedoption2\n",
        "    else:\n",
        "      first_search, second_search = option2, option1\n",
        "      first_mask, second_mask = maskedoption2, maskedoption1\n",
        "    \n",
        "    sentence = first_search.sub(first_mask, sentence) #Mask the longer word with OPTION_ mask\n",
        "    sentence = second_search.sub(second_mask, sentence) #then the shorter one\n",
        "\n",
        "    # IT gets kinda confusing which word now to <IGNORE> because it's language and some words appear more than two, more than three, times. \n",
        "    # I think the smartest approach is to assume the final word used is the question word and mask that one. \n",
        "\n",
        "    # Find final word used\n",
        "    # maskedoption1 is NOT the final word\n",
        "    if sentence.rfind(maskedoption1) < sentence.rfind(maskedoption2):\n",
        "      # IGNORE final word\n",
        "      # Convert other word back to original\n",
        "      sentence = sentence.replace(maskedoption1, unk)\n",
        "      sentence = sentence.replace(maskedoption2, p['option2'])\n",
        "    else:\n",
        "      sentence = sentence.replace(maskedoption2, unk)\n",
        "      sentence = sentence.replace(maskedoption1, p['option2'])\n",
        "\n",
        "\n",
        "    if toprint > 0:\n",
        "      print(f\"[{p['option1']}], [{p['option2']}], [{p['sentence']}]\")\n",
        "\n",
        "    sentences.append(sentence)\n",
        "    \n",
        "    if toprint > 0:\n",
        "      print(sentences[-1])\n",
        "      toprint -= 1\n",
        "\n",
        "  build = {'sentence':sentences, 'labels':dataset['labels'], 'option1':dataset['option1'], 'option2':dataset['option2']}\n",
        "  return Dataset.from_dict(build) # DON'T SHUFFLE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mask_datasets_2(dataset, tokenizer):\n",
        "  unk = tokenizer.unk_token\n",
        "  return {\"train\":mask_copy_2(dataset['train'], unk), \"test\":mask_copy_2(dataset['test'], unk), \"name\":\"Single <Unk> Masked Dataset\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stupid_tries = [\n",
        "  (\"dog\", \"doggy\"),\n",
        "  (\"red\", \"blue\"),\n",
        "  (\"flavor\", \"flavour\"),\n",
        "  ('A', 'B'),\n",
        "  ('X', 'Y'),\n",
        "  ('1', '2'),\n",
        "  ('first', 'second'),\n",
        "  ('alpha', 'beta'),\n",
        "  ('#', '@'),\n",
        "  ('primero', 'secundo'), # yes its true, I dont speak spanish. Having it spelled wrong just makes the model's task that much more difficult ;)\n",
        "  ('Alice', 'Bob'),\n",
        "  ('_', '__'),\n",
        "  ('mask1', 'mask2'),\n",
        "  ('thing1', 'thing2'),\n",
        "  ('mask_a', 'mask_b'),\n",
        "  (\"thing_a\", \"thing_b\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "uO3kaYKyosEQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# This masked <unk> tokens on the option which is NOT involved in the question. \n",
        "# If the option involved in the question is wrong, it will need to identify the <unk> token as having importance.\n",
        "def stupid_masking(dataset, mask1, mask2):\n",
        "  sentences = []\n",
        "  toprint = 1\n",
        "\n",
        "  for p in dataset:\n",
        "    # This is annoying because option1 can be substrings of other words, usually option2. They can also have uppercase letters. \n",
        "    # If one is a substring, then I will cover up the larger word with a temporary mask to not confuse anything else.\n",
        "    sentence = p['sentence']\n",
        "    option1, option2 = re.compile(p['option1'], re.IGNORECASE), re.compile(p['option2'], re.IGNORECASE)\n",
        "    maskedoption1, maskedoption2 = \"OPTION_ONE\", \"OPTION_TWO\"\n",
        "    first_search, second_search, first_mask, second_mask = None, None, None, None\n",
        "\n",
        "    # \"table\" in \"tablecloth\" --> cover bigger one\n",
        "    if len(p['option1']) > len(p['option2']):\n",
        "      # cover option1 first\n",
        "      first_search, second_search = option1, option2\n",
        "      first_mask, second_mask = maskedoption1, maskedoption2\n",
        "    else:\n",
        "      first_search, second_search = option2, option1\n",
        "      first_mask, second_mask = maskedoption2, maskedoption1\n",
        "    \n",
        "    sentence = first_search.sub(first_mask, sentence) #Mask the longer word with OPTION_ mask\n",
        "    sentence = second_search.sub(second_mask, sentence) #then the shorter one\n",
        "\n",
        "    sentence = sentence.replace(maskedoption1, mask1)\n",
        "    sentence = sentence.replace(maskedoption2, mask2)\n",
        "\n",
        "\n",
        "    if toprint > 0:\n",
        "      print(f\"[{p['option1']}], [{p['option2']}], [{p['sentence']}]\")\n",
        "\n",
        "    sentences.append(sentence)\n",
        "    \n",
        "    if toprint > 0:\n",
        "      print(sentences[-1])\n",
        "      toprint -= 1\n",
        "\n",
        "  build = {'sentence':sentences, 'labels':dataset['labels'], 'option1':dataset['option1'], 'option2':dataset['option2']}\n",
        "  return Dataset.from_dict(build) # DON'T SHUFFLE\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def stupid_datasets(dataset, tokenizer, masks):\n",
        "  mask1, mask2 = masks\n",
        "  return {\"train\":stupid_masking(dataset['train'], mask1, mask2), \"test\":stupid_masking(dataset['test'], mask1, mask2), \"name\":f\"({mask1}, {mask2}) Masked Dataset\"}"
      ],
      "metadata": {
        "id": "zWfGHxA7Z6td"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XkyD6UlObpsl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "# You can switch between these two datasets\n",
        "dataset = load_dataset(\"winogrande\", 'winogrande_debiased')\n",
        "#dataset = load_dataset(\"winogrande\", 'winogrande_l')\n",
        "\n",
        "# dataset is dict with keys ['train', 'test', 'validation']\n",
        "# Each with an enumerable of \n",
        "\"\"\"\n",
        "{'answer': '2',\n",
        " 'option1': 'Kyle',\n",
        " 'option2': 'Logan',\n",
        " 'sentence': \"Kyle doesn't wear leg warmers to bed, while Logan almost always does. _ is more likely to live in a colder climate.\"}\n",
        "\n",
        "\"\"\"\n",
        "# Use Validation instead of Test because Test lacks labels.\n",
        "\n",
        "\n",
        "#dicts of {'train':_, 'test':_}\n",
        "std_datasets = prepare_data(dataset)\n",
        "\n",
        "masked_datasets = mask_datasets(std_datasets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# This model will become the base model for everything we build upon, \n",
        "# so we will save it to disk to load it fresh for each experiment. \n",
        "model_list = [#\"DeepPavlov/roberta-large-winogrande\", \n",
        "              \"roberta-large\",\n",
        "              \"distilbert-base-uncased\", \n",
        "              \"albert-base-v2\", \n",
        "              'bert-base-cased', \n",
        "              'xlnet-base-cased', \n",
        "            ]\n",
        "\n",
        "for model_name_orig in model_list:\n",
        "  try:\n",
        "    print(\"NOW DOING\", model_name_orig)\n",
        "    # download model and save to memory. Models must be removed from cache to prevent gpu mem errors.\n",
        "    # fine tune the model to the debiased dataset\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_orig, model_max_length=64)\n",
        "    print(\"BEGIN OG TRAIN\")\n",
        "    debiased_tuned_model = train_model(trainingset=std_datasets, compareset=masked_datasets, tokenizer=tokenizer, model=model_name_orig, copy=False, epochs=3)\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "    print(\"SAVED\")\n",
        "    model_name = ''.join([c for c in model_name if c.isalpha()])\n",
        "    torch.save(debiased_tuned_model, f=model_name)\n",
        "    debiased_tuned_model = None\n",
        "\n",
        "\n",
        "    # Run individual tests \n",
        "    print(\"STARTING FIRST UNK\")\n",
        "    unk_datasets = mask_datasets_1(std_datasets, tokenizer)\n",
        "    test_datasets(tokenizer, std_datasets, unk_datasets, debiased_tuned_model)\n",
        "    train_model(unk_datasets, std_datasets, tokenizer, model=model_name, copy = True)\n",
        "    print(\"ENDING FIRST UNK & STARTING SECOND\")\n",
        "\n",
        "    unk_dataset_2 = mask_datasets_2(std_datasets, tokenizer)\n",
        "    test_datasets(tokenizer, std_datasets, unk_dataset_2, debiased_tuned_model)\n",
        "    train_model(unk_dataset_2, std_datasets, tokenizer, model=model_name, copy = True)\n",
        "\n",
        "    print(\"ENDING UNK TESTS AND STARTING STUPID MASKS\")\n",
        "\n",
        "    for masks in stupid_tries[-2:]:\n",
        "      stupid_ds = stupid_datasets(std_datasets, tokenizer, masks)\n",
        "      print(\"TESTING:\")\n",
        "      test_datasets(tokenizer, std_datasets, stupid_ds, model=model_name)\n",
        "      print(\"POST TRAINING:\")\n",
        "      train_model(stupid_ds, std_datasets, tokenizer, model=model_name, copy = True)\n",
        "\n",
        "  finally:\n",
        "    print(\"REMOVING MODEL\")\n",
        "    del tokenizer\n",
        "    os.remove(model_name)\n",
        "\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "id": "w6Wcrmu4oQXJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5f172c32de234248b6978535d1f74f77",
            "b433a6c60c304e30822fa3e44df6b7f4",
            "3ce4c91dec6c46379c0a11c8990410d9",
            "5979bb126ab846e181e7191009746fe4",
            "0578bc405abd4cdd943b1bcfe94a971d",
            "689c32d00a834f1386802c0554ab45e9",
            "bca44f86848744caa9e18eb7a529552b",
            "0c84b5ab988d4a1393a3256883d5541f",
            "dee0b5099a6b48dda1b0815eacbf5c54",
            "bdf29171f6a74a6a98c7f6b04a62b257",
            "a915a4d501764e9eb93c752878159d14"
          ]
        },
        "collapsed": true,
        "outputId": "2bb29ccb-c2ca-4c8a-b8c1-62d484b000ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOW DOING roberta-large\n",
            "BEGIN OG TRAIN\n",
            "\n",
            "\n",
            "TRAIN:\n",
            " Joel didn't want to let Logan play with the slot machine seeing as how Logan was on a hot streak.\n",
            "In geometry class Craig commented that Brett's nose is too big, resulting in Craig giving him a shiner.\n",
            "Making sure to not have any weeds in the garden was important to Victoria but not Elena because Elena entertained a lot in the garden.\n",
            "Maria asked Megan if she would help clean the garage because Megan wanted to get work done around the house.\n",
            "Wendy preferred to read the story than to watch the TV because the TV was quiet and wouldn't disturb anyone.\n",
            "James took the butter over the fire and poured it over the juice from the freezer. The butter is frozen.\n",
            "Eric found a lot of success in telemarketing products while Randy was reclusive since Randy was extroverted.\n",
            "Mike and friends practiced basketball out in the field instead of gym during rain, even though the gym is dry.\n",
            "Monica stretched their muscles before they worked out but Sarah didn't bother. Sarah hurt themselves during their gym session.\n",
            "Lindsey had a hard time waking up after midnight Sarah, because Lindsey was too gentle when they were calling.\n",
            "\n",
            "\n",
            "LABELS:\n",
            " 0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "\n",
            "\n",
            "TEST:\n",
            " James took out a seat from his office to create more space. The seat is too big.\n",
            "Ann liked using oil paint rather than acrylic on canvas because oil had a shortened working time.\n",
            "I couldn't decide if I should pressure cook or smoke the pig feet.  After looking outside, I decided the weather was poor enough to smoke them.\n",
            "The pine trees were still green in December, but the oak trees were not, because the oak trees lose their leaves in fall.\n",
            "Betty made the choice to reduce Samantha's salary after Betty started cutting back on efficiency.\n",
            "She couldn't wear the bra, unlike the shirt, because the shirt was the right size.\n",
            "It seemed more likely that Brett would wear a fedora rather than Justin because Justin did not often dress stylishly.\n",
            "It took a minute longer to melt the chocolate in the microwave than the caramel, because the caramel was very firm.\n",
            "I chose to drive the car over the truck because the truck had a small amount of gas.\n",
            "Christine had really good grammar but Cynthia didn't have good grammar because Cynthia studied math in college.\n",
            "\n",
            "\n",
            "LABELS:\n",
            " 1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "DOWNLOADING MODEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/19 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f172c32de234248b6978535d1f74f77"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1 / 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/578 [00:00<07:19,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "1 0\n",
            "0 0\n",
            "0 1\n",
            "1 0\n",
            "1 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "1 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/578 [00:01<07:12,  1.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "1 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "1 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 3/578 [00:02<07:09,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "1 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "1 0\n",
            "0 0\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 4/578 [00:02<07:08,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "1 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 5/578 [00:03<07:01,  1.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "0 0\n",
            "0 1\n",
            "1 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "1 1\n",
            "1 0\n",
            "1 0\n",
            "0 1\n",
            "1 0\n",
            "1 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 6/578 [00:04<07:06,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "1 1\n",
            "0 0\n",
            "0 1\n",
            "1 1\n",
            "1 0\n",
            "1 1\n",
            "1 0\n",
            "0 1\n",
            "1 0\n",
            "1 0\n",
            "1 0\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "1 1\n",
            "0 1\n",
            "1 0\n",
            "0 1\n",
            "1 0\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "1 1\n",
            "1 0\n",
            "1 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 7/578 [00:05<07:02,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 0\n",
            "1 1\n",
            "0 0\n",
            "1 1\n",
            "0 0\n",
            "1 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "1 0\n",
            "1 0\n",
            "0 1\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 8/578 [00:05<07:05,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1\n",
            "1 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "1 1\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "1 1\n",
            "1 0\n",
            "0 0\n",
            "1 1\n",
            "1 1\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "1 0\n",
            "0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 9/578 [00:06<07:03,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1\n",
            "0 0\n",
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "1 1\n",
            "1 1\n",
            "0 1\n",
            "1 0\n",
            "1 0\n",
            "1 0\n",
            "0 1\n",
            "1 0\n",
            "0 1\n",
            "1 0\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "0 1\n",
            "1 0\n",
            "0 0\n",
            "1 1\n",
            "1 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "1 1\n",
            "1 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 10/578 [00:07<07:02,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1\n",
            "0 0\n",
            "0 0\n",
            "1 0\n",
            "0 1\n",
            "0 1\n",
            "1 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 1\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 1\n",
            "0 0\n",
            "0 0\n",
            "0 0\n",
            "0 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 266/578 [03:08<03:40,  1.41it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "\n",
        "| Masking Scheme | Accuracy (before training) | Accuracy (after training on silly set) |\n",
        "|:-------:|:------------------:|:----------------:|\n",
        "|Standard set| 75% | 75% |\n",
        "|(Alice, Bob)| 73% | 73% |\n",
        "|(primero, secundo)| 72% | 75% |\n",
        "| (X, Y) | 72% | 74% |\n",
        "| (A, B) | 72% | 73% |\n",
        "| (1, 2) | 71% | 74% |\n",
        "|(red, blue) | 71% | 73% |\n",
        "|(#, @)| 71% | 73% |\n",
        "|(alpha, beta)| 71% | 73% |\n",
        "|(mask_a, mask_b) | 70% | 75% | \n",
        "|(thing_a, thing_b) | 70% | 73% |\n",
        "|(first, second)| 69% | 73% |\n",
        "|(\\_, \\_\\_)| 67% | 73% |\n",
        "|(dog, doggy)| 65% | 73% | \n",
        "|(flavor, flavour)| 53% | 74% |\n",
        "\n",
        "\n",
        "\n",
        "| Masking Scheme | Accuracy (before training) | Accuracy (after training on \\<unk\\> masked set) |\n",
        "|:-------:|:------------------:|:----------------:|\n",
        "|\\<unk\\> as subj| 65% | 73% |\n",
        "|\\<unk\\> as obj| 68% | 73% |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3k32Vfxq-YLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qNws0djclgtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Extension of Summerwinograndeattempt to more models",
      "provenance": [],
      "authorship_tag": "ABX9TyNNZFgeV75yy1DP+g0RGHpA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5f172c32de234248b6978535d1f74f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b433a6c60c304e30822fa3e44df6b7f4",
              "IPY_MODEL_3ce4c91dec6c46379c0a11c8990410d9",
              "IPY_MODEL_5979bb126ab846e181e7191009746fe4"
            ],
            "layout": "IPY_MODEL_0578bc405abd4cdd943b1bcfe94a971d"
          }
        },
        "b433a6c60c304e30822fa3e44df6b7f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_689c32d00a834f1386802c0554ab45e9",
            "placeholder": "​",
            "style": "IPY_MODEL_bca44f86848744caa9e18eb7a529552b",
            "value": "100%"
          }
        },
        "3ce4c91dec6c46379c0a11c8990410d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c84b5ab988d4a1393a3256883d5541f",
            "max": 19,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dee0b5099a6b48dda1b0815eacbf5c54",
            "value": 19
          }
        },
        "5979bb126ab846e181e7191009746fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdf29171f6a74a6a98c7f6b04a62b257",
            "placeholder": "​",
            "style": "IPY_MODEL_a915a4d501764e9eb93c752878159d14",
            "value": " 19/19 [00:01&lt;00:00, 13.79ba/s]"
          }
        },
        "0578bc405abd4cdd943b1bcfe94a971d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689c32d00a834f1386802c0554ab45e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bca44f86848744caa9e18eb7a529552b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c84b5ab988d4a1393a3256883d5541f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee0b5099a6b48dda1b0815eacbf5c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdf29171f6a74a6a98c7f6b04a62b257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a915a4d501764e9eb93c752878159d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}